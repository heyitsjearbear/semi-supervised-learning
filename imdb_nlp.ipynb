{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Learning Algorithm for Semi-Supervised Learning\n",
    "#### Steps taken from tutorial(check README)\n",
    " 1. Load data & preprocessing\n",
    " 2. Data splitting\n",
    " 3. Model training\n",
    " 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jereb\\anaconda3\\envs\\tensor\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\jereb\\anaconda3\\envs\\tensor\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\jereb\\anaconda3\\envs\\tensor\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import imdb\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(8000, 32, input_length=245))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_train_data(predictions, fold_n):\n",
    "    X_new = list()\n",
    "    y_new = list()\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        #our activation function will drop data when\n",
    "        #output is in between .05 and .95\n",
    "        if prediction > 0.95 or prediction < 0.05:\n",
    "            X_new.append(X_fold[fold_n][i])\n",
    "            y_new.append(np.argmax(prediction))\n",
    "    \n",
    "    return np.array(X_new), np.array(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_shuffle(X_train, y_train, X_new, y_new):\n",
    "    X_train = np.vstack((X_train, X_new))\n",
    "    y_train = np.append(y_train, y_new)\n",
    "    \n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return X_train[indices], y_train[indices]\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & preprocessing\n",
    "First we are going to load all of our required modules and dataset. We will be using IMDB movie reviews. With our model we are going to determine if a text is negative or positive review (sentiment analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuple collection of unchangeable, ordered data,\n",
    "#going to store dataset into tuples here with load_data function\n",
    "(X, y), (X_test, y_test) = imdb.load_data(num_words=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thisTuple = (1,2)\n",
    "# (z,f) = ((1,2),(4,5))\n",
    "# for x in z:\n",
    "#     print(x)\n",
    "# for y in f:\n",
    "#     print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see tuples\n",
    "# print(X[0])\n",
    "# print(X_test[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every review has different amount of words, which is a problem because model will expect a fixed input size.\n",
    "Solve this with pad_sequences() function from keras. which will limit text length.\n",
    "Below will iterate over movie reviews, find the average, and limit text that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.71364\n"
     ]
    }
   ],
   "source": [
    "reviewLengths = list()\n",
    "for x in X:\n",
    "    reviewLengths.append(len(x))\n",
    "\n",
    "print(np.mean(reviewLengths))\n",
    "\n",
    "X_pad = pad_sequences(X,maxlen=245)\n",
    "X_test_pad = pad_sequences(X_test,maxlen=245)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "* With this project all of the reviews are labeled. To simulate semi-supervised learning, then the assumption is that part of the reviews are labeled. We are going to train the model on small amount of data, and with high confidentiality from this, we are going to use this and the previous 'unlabeled' data to retrain model.\n",
    "* To split data with a *kFold* object from Sklearn. We are going to split data into 5 different folds, and fold 0 will be our 'first go' at training our model because it is assumed fold 0 is our only labeled data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 5 folds, shuffle data before dividing data into folds\n",
    "kf = KFold(n_splits=5, shuffle=True) \n",
    "\n",
    "X_fold = list()\n",
    "y_fold = list()\n",
    "\n",
    "#loop over kf and split into different folds\n",
    "for _, fold in kf.split(X_pad):\n",
    "    X_fold.append(X_pad[fold])\n",
    "    y_fold.append(y[fold])\n",
    "\n",
    "#storefolds into numpy array\n",
    "X_fold = np.array(X_fold)\n",
    "y_fold = np.array(y_fold)\n",
    "\n",
    "# The 0th fold will be our known labeled, the rest folds are assumed to be unlabeled\n",
    "X_train = X_fold[0]\n",
    "y_train = y_fold[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "* Done using LSTM-Based neural network **(LSTM)**. LSTM is a type of RNN. What is RNN? It is a type of artificial neural network that take information from previous inputs to influence current input and output. RNN's output depend on prior sequential elements. \n",
    "* LSTM tries to mimick the human brain, handling previous knowledge to handle learning long-term dependencies in short-term data, suitable for NLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "* data folds stored in X_fold[0] to X_fold[4].\n",
    "* X_train and y_train is taken from first fold array and pretend its our only labeled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## going to initialize model and apply fit. \n",
    "## Use last 1000 reviews instead of all 5000.\n",
    "* to see if model overfits(happens when model cannot perform accurately against unseen data)\n",
    "* use 2 epochs for see overfitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "125/125 [==============================] - 6s 18ms/step - loss: 0.6751 - acc: 0.5785 - val_loss: 0.5967 - val_acc: 0.7560\n",
      "Epoch 2/2\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.4218 - acc: 0.8255 - val_loss: 0.4172 - val_acc: 0.8060\n",
      "157/157 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(X_train[:-1000], y_train[:-1000], epochs=2, \n",
    "          validation_data=(X_train[-1000:], y_train[-1000:]))\n",
    "          \n",
    "predictions = model.predict(X_fold[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this model is semi trained, in order to better model we must train rest of folds and use those predictions as more labeled data\n",
    "* **This isn't the best idea though**\n",
    "* perform filtering method called **pseudo-labeling**\n",
    "* this will result predictions with low confidentiality score to be dropped from folds since these have a chance of not being correct prediction-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join new data with existing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = get_new_train_data(predictions, 1)\n",
    "X_train, y_train = join_shuffle(X_train, y_train, X_new, y_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "131/131 [==============================] - 5s 23ms/step - loss: 0.6648 - acc: 0.5941 - val_loss: 0.5937 - val_acc: 0.7670\n",
      "Epoch 2/3\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.4748 - acc: 0.8135 - val_loss: 0.5842 - val_acc: 0.7780\n",
      "Epoch 3/3\n",
      "131/131 [==============================] - 2s 15ms/step - loss: 0.3592 - acc: 0.8742 - val_loss: 0.4875 - val_acc: 0.7770\n",
      "157/157 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.fit(X_train[:-1000], y_train[:-1000], epochs=3, \n",
    "          validation_data=(X_train[-1000:], y_train[-1000:]))\n",
    "\n",
    "# Predict samples in fold 2\n",
    "predictions = model.predict(X_fold[2])\n",
    "\n",
    "# Filter out samples in fold 2\n",
    "X_new, y_new = get_new_train_data(predictions, 2)\n",
    "\n",
    "# Concatenate new data to X_train and y_train\n",
    "X_train, y_train = join_shuffle(X_train, y_train, X_new, y_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use data from fold 0,1,2 to train new model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "170/170 [==============================] - 7s 40ms/step - loss: 0.3241 - acc: 0.8843 - val_loss: 0.4256 - val_acc: 0.8180\n",
      "Epoch 2/2\n",
      "170/170 [==============================] - 7s 40ms/step - loss: 0.2300 - acc: 0.9247 - val_loss: 0.4585 - val_acc: 0.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d33600700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=2, \n",
    "          validation_data=(X_test_pad, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46d6a5147c76bd4613d60e17725a3da408dded3f5ad5c56df4196e799c49dd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
